{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "class SparkSessionBases(object):\n",
    " \n",
    "    SPARK_APP_NAME = None \n",
    "    SPARK_URL = 'yarn'\n",
    "    SPARK_EXECUTOR_MEMORY = '2g' \n",
    "    SPARK_EXECUTOR_CORES = 2\n",
    "    SPARK_EXECUTOR_INSTANCES = 2\n",
    "    ENABLE_HIVE_SUPPORT = False\n",
    "\n",
    "    def _create_spark_session(self):\n",
    "        conf = SparkConf()\n",
    "        config = (\n",
    "            ('spark.app.name',self.SPARK_APP_NAME),\n",
    "            ('spark.executor.memory',self.SPARK_EXECUTOR_MEMORY),\n",
    "            ('spark.master',self.SPARK_URL),\n",
    "            ('spark.executor.cores',self.SPARK_EXECUTOR_CORES),\n",
    "            ('spark.executor.instances',self.SPARK_EXECUTOR_INSTANCES),\n",
    "            )\n",
    "        conf.setAll(config)\n",
    "        if self.ENABLE_HIVE_SUPPORT:\n",
    "            return SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()\n",
    "        else:\n",
    "            return SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.ml.feature import CountVectorizer,IDF\n",
    "from pyspark.ml.feature import CountVectorizerModel\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0,os.path.join(BASE_DIR))\n",
    "PYSPARK_PYTHON = '/miniconda2/envs/reco_sys/bin/python'\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = PYSPARK_PYTHON\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = PYSPARK_PYTHON\n",
    "\n",
    "# from config import SparkSessionBases\n",
    "class KeywordsToTfidf(SparkSessionBases):\n",
    "    SPARK_APP_NAME = 'keywordByTFIDF'\n",
    "    SPARK_URL = 'spark://master:7077'\n",
    "    ENABLE_HIVE_SUPPORT = True\n",
    "    SPARK_EXECUTOR_MEMORY = '6g'\n",
    "    SPARK_EXECUTOR_CORES = 4\n",
    "    SPARK_EXECUTOR_INSTANCES = 8\n",
    "\n",
    "    def __init__(self):\n",
    "        self.spark = self._create_spark_session()\n",
    "\n",
    "ktt = KeywordsToTfidf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ktt.spark.sql('use article')\n",
    "\n",
    "\n",
    "def segmentation(partition):\n",
    "    import os\n",
    "    import re\n",
    "    import jieba\n",
    "    import jieba.analyse\n",
    "    import jieba.posseg\n",
    "    import jieba.posseg as pseg\n",
    "    import codecs\n",
    "\n",
    "    abspath = '/root/words'\n",
    "    userDict_path = os.path.join(abspath,'ITKeywords.txt')\n",
    "    jieba.load_userdict(userDict_path)\n",
    "\n",
    "    stopwords_path = os.path.join(abspath,\"stopwords.txt\")\n",
    "    def get_stopwords_list():\n",
    "        stopwords_list = [i.strip() for i  in codecs.open(stopwords_path).readline()]\n",
    "        return stopwords_list\n",
    "    stopwords_list = get_stopwords_list()\n",
    "\n",
    "    def cut_sentence(sentence):\n",
    "        seg_list = pseg.lcut(sentence)\n",
    "        seg_list = [i for i in seg_list if i.flag not in stopwords_list]\n",
    "        filtered_word_list = []\n",
    "        for seg in seg_list:\n",
    "            if len(seg.word) <= 1:\n",
    "                continue\n",
    "            elif seg.flag == 'eng':\n",
    "                if len(seg.word) <= 2:\n",
    "                    continue\n",
    "                else:\n",
    "                    filtered_word_list.append(seg.word)\n",
    "            elif seg.flag.startswith('n'):\n",
    "                filtered_word_list.append(seg.word)\n",
    "            elif seg.flag in ['x','eng']:\n",
    "                filtered_word_list.append(seg.word)\n",
    "        return filtered_word_list\n",
    "\n",
    "    for row in partition:\n",
    "        sentence = re.sub(\"<.*?>\",\"\",row.sentence)\n",
    "        words = cut_sentence(sentence)\n",
    "        yield row.article_id,row.channel_id,words\n",
    "\n",
    "article_dataframe = ktt.spark.sql('select * from article_data')\n",
    "words_df = article_dataframe.rdd.mapPartitions(segmentation).toDF(['article_id','channel_id','words'])\n",
    "# print(words_df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[article_id: bigint, channel_id: bigint, words: array<string>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer_49d688d848df461899b4\n"
     ]
    }
   ],
   "source": [
    "cv_model = CountVectorizerModel.load(\"hdfs://master:9000/headlines/model/CV.model\")\n",
    "print(cv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result = cv_model.transform(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[article_id: bigint, channel_id: bigint, words: array<string>, countFeatures: vector]\n"
     ]
    }
   ],
   "source": [
    "print(cv_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import IDF\n",
    "# idf = IDF(inputCol=\"countFeatures\", outputCol=\"idfFeatures\")\n",
    "# idfModel = idf.fit(cv_result)\n",
    "# idfModel.write().overwrite().save(\"hdfs://master:9000/headlines/models/IDF.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF_46ba8959a2dc62a6a6f0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDFModel\n",
    "idf_model = IDFModel.load(\"hdfs://master:9000/headlines/model/IDF.model\")\n",
    "print(idf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view of cv_model\n",
    "# print(type(cv_model))\n",
    "# help(cv_model)\n",
    "# cv_model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.41793052,  0.66523945,  0.80716005, ..., 11.14709445,\n",
       "       11.14709445, 11.14709445])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_model.idf.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list_with_idf = list(zip(cv_model.vocabulary, idf_model.idf.toArray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list_with_idf_1 = keywords_list_with_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(keywords_list_with_idf_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spark\n",
    "def func(data):\n",
    "    for index in range(len(data)):\n",
    "        data[index] = list(data[index])\n",
    "        data[index].append(index)\n",
    "        data[index][1] = float(data[index][1])\n",
    "func(keywords_list_with_idf)\n",
    "sc = ktt.spark.sparkContext\n",
    "rdd = sc.parallelize(keywords_list_with_idf)\n",
    "df = rdd.toDF([\"keywords\", \"idf\", \"index\"])\n",
    "\n",
    "df.write.insertInto('idf_keywords_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[article_id: bigint, channel_id: bigint, words: array<string>, countFeatures: vector, idfFeatures: vector]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_result = idf_model.transform(cv_result)\n",
    "tfidf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+--------------------+\n",
      "|article_id|channel_id|               words|       countFeatures|\n",
      "+----------+----------+--------------------+--------------------+\n",
      "|         1|        17|[Vue, props, 用法, ...|(1234576,[1,2,3,5...|\n",
      "+----------+----------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_result.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+--------------------+--------------------+\n",
      "|article_id|channel_id|               words|       countFeatures|         idfFeatures|\n",
      "+----------+----------+--------------------+--------------------+--------------------+\n",
      "|         1|        17|[Vue, props, 用法, ...|(1234576,[1,2,3,5...|(1234576,[1,2,3,5...|\n",
      "+----------+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_result.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_result.describe(['channel_id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(article_id=1, channel_id=17, words=['Vue', 'props', '用法', '小结', 'Vue', 'props', '用法', '组件', '选项', 'props', 'Vue', '选项', '父子', '组件', '关系', '总结', 'props', 'down', 'events', '组件', 'props', '传递数据', '组件', '组件', 'events', '组件', '发送消息', '父子', '组件', '组件', 'pa', 'rent', 'child', '组件', '环境', '书写', '组件', '可维护性', '定义', '父子', '组件', 'Vue', '对象', 'var', 'childNode', 'template', 'div', 'childNode', 'div', 'var', 'pa', 'rentNode', 'template', 'div', 'child', 'child', 'child', 'child', 'div', 'components', 'child', 'childNode', '全栈', '交流', 'Ian', '人员', '技术', '瓶颈', '思维能力', 'new', 'Vue', 'example', 'components', 'pa', 'rent', 'pa', 'rentNode', 'div', 'example', 'pa', 'rent', 'pa', 'rent', 'div', 'childNode', '定义', 'template', 'div', '内容', 'childNode', '字符串', 'pa', 'rentNode', 'template', '定义', 'div', 'class', 'pa', 'rent', 'child', '组件', '静态', 'props', '组件', '实例', '作用域', '组件', '模板', '饮用', '组件', '数据', '组件', '组件', '数据', '组件', 'props', '选项', '组件', '向子', '组件', '传递数据', '方式', '动态', '静态', '静态', '方式', '组件', 'props', '声明', '数据', '上例', '代码', 'childNode', 'props', '选项', 'forChildMsg', '数据', '组件', '占位符', '特性', '方式', '传递数据', 'var', 'childNode', 'template', 'div', 'forChildMsg', 'div', 'props', 'for', 'child', 'msg', 'var', 'pa', 'rentNode', 'template', 'div', 'pa', 'rentNode', 'child', 'for', 'child', 'msg', 'aaa', 'child', 'child', 'for', 'child', 'msg', 'bbb', 'child', 'div', 'components', 'child', 'childNode', '命名规范', 'props', '声明', '属性', '组件', 'template', '模板', '属性', '中划线', '组件', 'props', '属性', '声明', '驼峰', '中划线', '组件', '模板', '组件', '驼峰', 'Vue', '驼峰', '命名', 'forChildMsg', 'for', 'child', 'msg', '动态', 'props', '模板', '动态', '组件', '数据', '组件', '模板', 'props', 'Html', '标签', '特性', 'bind', '静态', 'props', '代码', '组件', 'var', 'pa', 'rentNode', 'template', 'div', 'pa', 'rentNode', 'child', 'for', 'child', 'msg', 'childMsg1', 'child', 'child', 'for', 'child', 'msg', 'childMsg2', 'child', 'div', 'components', 'child', 'childNode', 'data', 'function', 'return', 'childMsg1', 'Dynamic', 'props', 'msg', 'for', 'child', 'childMsg2', 'Dynamic', 'props', 'msg', 'for', 'child', '组件', 'data', 'return', '数据', 'childMsg1', 'childMsg2', '组件', 'props', 'props', '参数', '数据', '规格', '数据', '规格', 'Vue', '警告', '种类', 'type', 'String', 'Number', 'Boolean', 'Function', 'Object', 'Array', 'SymbolVue', 'component', 'example', 'props', '基础', '类型', 'ul', '类型', '都行', 'propA', 'Number', '多种类型', 'propB', 'String', 'Number', 'String', 'propC', 'type', 'String', 'required', 'true', '数字', 'propD', 'type', 'Number', 'defa', 'ul', '数组', '工厂', '函数返回', '对象', 'propE', 'type', 'Object', 'defa', 'ul', 'function', 'console', 'log', 'propE', 'defa', 'ul', 'invoked', 'return', 'message', 'from', 'propE', '函数', 'propF', 'isValid', 'function', 'value', 'return', 'value', 'let', 'childNode', 'template', 'div', 'forChildMsg', 'div', 'props', 'for', 'child', 'msg', 'Number', 'let', 'pa', 'rentNode', 'template', 'div', 'class', 'pa', 'rent', 'child', 'for', 'child', 'msg', 'msg', 'child', 'div', 'components', 'child', 'childNode', 'data', 'return', '字符串', '时会', 'msg', 'props', '定义', '数据', '函数', '函数返回', 'false', '警告', '例子', 'childNode', 'for', 'child', 'msg', '一个对象', 'validator', '函数', '命名', '规定', 'validator', '自定义函数', '生效', 'let', 'childNode', 'template', 'div', 'forChildMsg', 'div', 'props', 'for', 'child', 'msg', 'validator', 'function', 'value', 'return', 'value', 'for', 'child', 'msg', 'validator', '函数', '警告', '单向', '数据流', 'props', '单向', '组件', '属性', '传导', '组件', '组件', '组件', '状态', '组件', 'props', 'Vue', '警告', 'let', 'childNode', 'template', 'div', 'class', 'child', 'div', 'pa', '组件', '数据', 'pa', 'input', 'model', 'forChildMsg', 'div', 'forChildMsg', 'div', 'props', 'for', 'child', 'msg', 'String', 'let', 'pa', 'rentNode', 'template', 'div', 'class', 'pa', 'rent', 'div', 'pa', '组件', '数据', 'pa', 'input', 'model', 'msg', 'div', 'msg', 'child', 'for', 'child', 'msg', 'msg', 'child', 'div', 'components', 'child', 'childNode', 'data', 'return', 'msg', 'defa', 'ul', 'string', '组件', '和子', '组件', '输入框', '组件', '数据', '和子', '组件', '数据', '组件', '输入框', '数据', '组件', '数据', 'props', '向子', '组件', '传递数据', '组件', '输入框', '浏览器', '错误', '警告', 'Vue', 'warn', 'Avoid', 'mutating', 'prop', 'directly', 'since', 'the', 'value', 'will', 'overwritten', 'whenever', 'the', 'pa', 'rent', 'component', 'renders', 'Instead', 'use', 'data', 'computed', 'property', 'based', 'the', 'prop', 'value', 'Prop', 'being', 'mutated', 'forChildMsg', 'props', '数据', '原因', 'prop', '组件', '局部', '数据', 'prop', '由子', '组件', '数据', '办法', '定义', '局部变量', 'prop', '定义', 'ownChildMsg', 'forChildMsg', '组件', 'ownChildMsg', '无法', 'childNode', 'template', 'div', 'class', 'child', 'div', 'pa', '组件', '数据', 'pa', 'input', 'model', 'forChildMsg', 'div', 'forChildMsg', 'ownChildMsg', 'ownChildMsg', 'div', 'props', 'for', 'child', 'msg', 'String', 'data', 'return', 'ownChildMsg', 'this', 'forChildMsg', 'ownChildMsg', '数据', '结果', 'ownChildMsg', '组件', 'forChildMsg', 'ownChildMsg', '定义', '属性', 'prop', '属性', '组件', 'forChildMsg', '数据', 'forChildMsg', '字符串', 'ownChildMsg', '屏幕', '组件', '数据', 'ownChildMsg', '数据', 'let', 'childNode', 'template', 'div', 'class', 'child', 'div', 'pa', '组件', '数据', 'pa', 'input', 'model', 'forChildMsg', 'div', 'forChildMsg', 'ownChildMsg', 'ownChildMsg', 'div', 'props', 'for', 'child', 'msg', 'String', 'computed', 'ownChildMsg', 'return', 'this', 'forChildMsg', 'ownChildMsg', '方式', 'prop', 'watch', 'prop', 'let', 'childNode', 'template', 'div', 'class', 'child', 'div', 'pa', '组件', '数据', 'pa', 'input', 'model', 'forChildMsg', 'div', 'forChildMsg', 'ownChildMsg', 'ownChildMsg', 'div', 'props', 'for', 'child', 'msg', 'String', 'data', 'return', 'ownChildMsg', 'this', 'forChildMsg', '全栈', '交流', '人员', '技术', '瓶颈', '思维能力', 'watch', 'forChildMsg', 'this', 'ownChildMsg', 'this', 'forChildMsg'], countFeatures=SparseVector(1234576, {1: 27.0, 2: 5.0, 3: 2.0, 5: 25.0, 6: 10.0, 7: 2.0, 9: 5.0, 12: 1.0, 14: 4.0, 16: 7.0, 21: 4.0, 23: 8.0, 24: 1.0, 26: 5.0, 27: 1.0, 28: 18.0, 29: 6.0, 30: 4.0, 32: 7.0, 34: 3.0, 35: 2.0, 36: 6.0, 37: 1.0, 39: 1.0, 40: 7.0, 41: 2.0, 45: 40.0, 56: 1.0, 58: 4.0, 60: 1.0, 62: 55.0, 64: 1.0, 68: 1.0, 74: 1.0, 78: 3.0, 83: 1.0, 87: 1.0, 92: 1.0, 102: 1.0, 110: 1.0, 117: 7.0, 121: 2.0, 131: 1.0, 139: 5.0, 141: 1.0, 149: 1.0, 153: 1.0, 158: 1.0, 179: 4.0, 184: 1.0, 189: 1.0, 198: 3.0, 207: 1.0, 233: 1.0, 304: 2.0, 314: 8.0, 322: 1.0, 341: 1.0, 342: 5.0, 343: 1.0, 347: 9.0, 391: 24.0, 392: 4.0, 399: 3.0, 424: 5.0, 492: 16.0, 515: 45.0, 530: 1.0, 538: 1.0, 574: 2.0, 583: 1.0, 591: 31.0, 594: 3.0, 629: 1.0, 634: 2.0, 644: 1.0, 717: 4.0, 742: 2.0, 751: 1.0, 800: 2.0, 802: 2.0, 848: 1.0, 965: 1.0, 1021: 1.0, 1060: 5.0, 1064: 1.0, 1111: 1.0, 1156: 1.0, 1163: 1.0, 1239: 1.0, 1321: 1.0, 1353: 6.0, 1381: 1.0, 1465: 8.0, 1573: 2.0, 1656: 1.0, 1697: 1.0, 1730: 5.0, 1826: 2.0, 1830: 2.0, 2116: 1.0, 2214: 1.0, 2246: 2.0, 2326: 1.0, 2338: 3.0, 2354: 1.0, 2603: 1.0, 2650: 1.0, 2755: 2.0, 2815: 1.0, 3309: 2.0, 3359: 1.0, 3438: 1.0, 3479: 2.0, 4375: 2.0, 4758: 3.0, 4780: 1.0, 4861: 1.0, 4964: 3.0, 5154: 1.0, 5362: 4.0, 5647: 9.0, 5805: 4.0, 5982: 1.0, 7279: 1.0, 7746: 2.0, 7758: 1.0, 9610: 1.0, 9934: 1.0, 10190: 1.0, 10927: 1.0, 12026: 2.0, 12277: 1.0, 13785: 1.0, 14691: 2.0, 16854: 1.0, 19369: 1.0, 19752: 1.0, 19892: 1.0, 20162: 18.0, 21392: 1.0, 21633: 2.0, 24205: 1.0, 28303: 2.0, 28704: 1.0, 35259: 1.0, 42362: 1.0, 45926: 1.0, 47550: 1.0, 49239: 1.0, 63079: 1.0, 88481: 1.0, 98468: 23.0, 118420: 18.0, 119641: 1.0, 125366: 1.0, 143271: 1.0, 144609: 3.0, 398644: 3.0, 481454: 3.0, 1187747: 1.0}), idfFeatures=SparseVector(1234576, {1: 17.9615, 2: 4.0358, 3: 1.4739, 5: 23.4444, 6: 11.586, 7: 2.5536, 9: 8.1242, 12: 1.3336, 14: 5.6582, 16: 9.4945, 21: 7.1656, 23: 15.3931, 24: 0.745, 26: 9.3677, 27: 1.265, 28: 22.5749, 29: 9.159, 30: 3.7359, 32: 12.9907, 34: 5.8238, 35: 3.1358, 36: 11.2438, 37: 1.1584, 39: 2.0028, 40: 8.9766, 41: 2.8313, 45: 101.2493, 56: 1.6358, 58: 7.429, 60: 2.0406, 62: 125.4204, 64: 2.4321, 68: 1.7527, 74: 1.8912, 78: 5.8838, 83: 1.6398, 87: 1.8538, 92: 1.9374, 102: 2.0288, 110: 2.4065, 117: 19.7102, 121: 4.8307, 131: 1.6958, 139: 12.9688, 141: 1.7992, 149: 1.6676, 153: 2.4016, 158: 1.6351, 179: 9.0317, 184: 2.1673, 189: 1.9195, 198: 6.6922, 207: 1.9587, 233: 1.8041, 304: 4.4333, 314: 26.153, 322: 3.5174, 341: 3.1962, 342: 16.5566, 343: 2.643, 347: 31.9541, 391: 85.5751, 392: 10.9602, 399: 7.2229, 424: 14.8465, 492: 56.6672, 515: 174.1587, 530: 3.0759, 538: 3.5308, 574: 5.3271, 583: 3.3547, 591: 126.9927, 594: 10.2852, 629: 2.8597, 634: 5.8092, 644: 3.4414, 717: 12.1515, 742: 6.089, 751: 2.6767, 800: 5.6751, 802: 7.9865, 848: 3.4315, 965: 3.2083, 1021: 3.9508, 1060: 19.7203, 1064: 3.8705, 1111: 3.0659, 1156: 3.3593, 1163: 4.0162, 1239: 3.7538, 1321: 4.1193, 1353: 25.282, 1381: 4.0512, 1465: 37.7843, 1573: 9.1566, 1656: 3.6641, 1697: 3.8463, 1730: 19.6943, 1826: 8.9769, 1830: 7.8057, 2116: 4.4161, 2214: 4.3643, 2246: 8.4186, 2326: 4.0158, 2338: 13.4963, 2354: 4.723, 2603: 4.4203, 2650: 4.5111, 2755: 10.183, 2815: 4.674, 3309: 9.6491, 3359: 4.9986, 3438: 4.7738, 3479: 9.4091, 4375: 10.2137, 4758: 15.2325, 4780: 5.4981, 4861: 4.8875, 4964: 15.4001, 5154: 5.1358, 5362: 20.4897, 5647: 51.6391, 5805: 23.9984, 5982: 5.5141, 7279: 5.6337, 7746: 11.3581, 7758: 5.5376, 9610: 5.6918, 9934: 5.731, 10190: 5.654, 10927: 6.1232, 12026: 11.9818, 12277: 5.8314, 13785: 6.7103, 14691: 12.9023, 16854: 7.3404, 19369: 6.8033, 19752: 6.7904, 19892: 7.4708, 20162: 141.6591, 21392: 6.8164, 21633: 13.8848, 24205: 6.935, 28303: 14.6369, 28704: 7.2153, 35259: 7.5636, 42362: 7.8329, 45926: 8.7047, 47550: 7.9901, 49239: 7.8149, 63079: 8.3437, 88481: 9.007, 98468: 256.3832, 118420: 200.6477, 119641: 9.1322, 125366: 9.1322, 143271: 9.2753, 144609: 28.066, 398644: 33.4413, 481454: 33.4413, 1187747: 11.1471}))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_result.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         idfFeatures|\n",
      "+--------------------+\n",
      "|(1234576,[1,2,3,5...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_result.select('idfFeatures').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(partition):\n",
    "    TOPK=20\n",
    "    for row in partition:\n",
    "        _ = list(zip(row.idfFeatures.indices,row.idfFeatures.values))\n",
    "        _ = sorted(_,key=lambda x:x[1],reverse=True)\n",
    "        result = _[:TOPK]\n",
    "        for word_index, tfidf in result:\n",
    "            yield row.article_id,row.channel_id ,int(word_index),round(float(tfidf),4)\n",
    "_keywordsByTFIDF = tfidf_result.rdd.mapPartitions(func).toDF(['article_id','channel_id','index','tfidf'])\n",
    "# _keywordsByTFIDF = tfidf_result.rdd.mapPartitions(func).toDF(['article_id','channel_id','index','tfidf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- article_id: long (nullable = true)\n",
      " |-- channel_id: long (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- countFeatures: vector (nullable = true)\n",
      " |-- idfFeatures: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_result.limit(1).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+--------+\n",
      "|article_id|channel_id| index|   tfidf|\n",
      "+----------+----------+------+--------+\n",
      "|         1|        17| 98468|256.3832|\n",
      "|         1|        17|118420|200.6477|\n",
      "|         1|        17|   515|174.1587|\n",
      "|         1|        17| 20162|141.6591|\n",
      "|         1|        17|   591|126.9927|\n",
      "|         1|        17|    62|125.4204|\n",
      "|         1|        17|    45|101.2493|\n",
      "|         1|        17|   391| 85.5751|\n",
      "|         1|        17|   492| 56.6672|\n",
      "|         1|        17|  5647| 51.6391|\n",
      "+----------+----------+------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_keywordsByTFIDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------------+\n",
      "|  a|  b|                   c|\n",
      "+---+---+--------------------+\n",
      "|  1| 17|[Vue, props, 用法, ...|\n",
      "|  2| 17|[vue, 响应式, 原理, mo...|\n",
      "|  3| 17|[JavaScript, 浅拷贝,...|\n",
      "|  4| 17|[vue2, vuex, elem...|\n",
      "|  5| 17|[immutability, Re...|\n",
      "|  6| 17|[node, npm, cnpm,...|\n",
      "|  7| 17|[Web, 工程师, 以太坊, 入...|\n",
      "|  8| 17|[Web, pa, api, we...|\n",
      "|  9| 17|[vue, 中用, 数据驱动, 视...|\n",
      "| 10| 17|[程序, WebSocket, 长...|\n",
      "| 11| 17|[flux, 架构, flux, ...|\n",
      "| 12| 17|[合格, TypeScript, ...|\n",
      "| 13| 17|[专属, 插件, Easy, Sl...|\n",
      "| 14| 17|[前后端分离, vue, 网站前台...|\n",
      "| 15| 17|[ajax, 页面, 重复提交, ...|\n",
      "| 17| 17|[JSsearch, 购物网站, ...|\n",
      "| 18| 17|[web, pa, react, ...|\n",
      "| 19| 17|[合格, 事顶, 项目, 自我介绍...|\n",
      "| 20| 17|[总结, jQuery, 用法, ...|\n",
      "| 21| 17|[Bootstrap, Modal...|\n",
      "+---+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "164.63382411003113\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.types import StructField,StringType,FloatType,StructType\n",
    "# import time\n",
    "# start = time.time()\n",
    "# tfidf_result.limit(100).rdd.map(lambda x:(x[0],x[1],x[2])).toDF(['a','b','c']).show()\n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordsIndex = ktt.spark.sql('select keyword,index idx from idf_keywords_values')\n",
    "keywordsByTFIDF = _keywordsByTFIDF.join(keywordsIndex,keywordsIndex.idx ==_keywordsByTFIDF.index).select(['article_id','channel_id','keyword','tfidf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+-------+\n",
      "|article_id|channel_id|keyword|  tfidf|\n",
      "+----------+----------+-------+-------+\n",
      "|         3|        17|    var|22.4825|\n",
      "+----------+----------+-------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keywordsByTFIDF.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordsByTFIDF.write.insertInto('tfidf_keywords_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank(partition):\n",
    "    import os\n",
    "    import jieba\n",
    "    import jieba.analyse\n",
    "    import jieba.posseg as pesg\n",
    "    import codecs\n",
    "    abspath = '/root/words'\n",
    "    \n",
    "    userDict_path = os.path.join(abspath,'ITKeywords.txt')\n",
    "    jieba.load_userdict(userDict_path)\n",
    "    \n",
    "    stopwords_path = os.path.join(abspath,'stopwords.txt')\n",
    "    \n",
    "    def get_stopwords_list():\n",
    "        stopwords_list = [i.strip() for i in codecs.open(stopwords_path).readlines()]\n",
    "        return stopwords_list\n",
    "    stopwords_list = get_stopwords_list()\n",
    "    \n",
    "    class TextRank(jieba.analyse.TextRank):\n",
    "        def __init__(self,window=20,word_min_len=2):\n",
    "            super(TextRank,self).__init__()\n",
    "            self.span = window\n",
    "            self.word_min_len = word_min_len\n",
    "            self.pos_filt = frozenset(('n', 'x', 'eng', 'f', 's', 't', 'nr', 'ns', 'nt', \"nw\", \"nz\", \"PER\", \"LOC\", \"ORG\"))\n",
    "        def pairfilter(self,wp):\n",
    "            if wp.flag == 'eng':\n",
    "                if len(wp.word) <= 2:\n",
    "                    return False\n",
    "            if wp.flag in self.pos_filt and len(wp.word.strip()) >= self.word_min_len and wp.word.lower() not in stopwords_list:\n",
    "                return True\n",
    "    textrank_model = TextRank(window=5,word_min_len=2)\n",
    "    allowPOS = ('n', \"x\", 'eng', 'nr', 'ns', 'nt', \"nw\", \"nz\", \"c\")\n",
    "    \n",
    "    for row in partition:\n",
    "        tags = textrank_model.textrank(row.sentence,topK=20,withWeight=True,allowPOS=allowPOS,withFlag=False)\n",
    "        for tag in tags:\n",
    "            yield row.article_id,row.channel_id,tag[0],tag[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank_keywords_df = article_dataframe.rdd.mapPartitions(textrank).toDF(['article_id','channel_id','keyword','textrank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank_keywords_df.write.insertInto('textrank_keywords_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = ktt.spark.sql('select * from idf_keywords_values')\n",
    "idf = idf.withColumnRenamed('keyword','keyword1')\n",
    "result = textrank_keywords_df.join(idf,textrank_keywords_df.keyword == idf.keyword1)\n",
    "keywords_res = result.withColumn('weights',result.textrank*result.idf).select(['article_id','channel_id','keyword','weights'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_res.registerTempTable('temptable')\n",
    "merge_keywords = ktt.spark.sql('select article_id,min(channel_id) channel_id,collect_list(keyword) keywords,collect_list(weights) weights from temptable group by article_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _func(row):\n",
    "    return row.article_id,row.channel_id,dict(zip(row.keywords,row.weights))\n",
    "keywords_info = merge_keywords.rdd.map(_func).toDF(['article_id','channel_id','keywords'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sql = \"\"\"select t.article_id article_id2,collect_set(t.keywords) topics from tfidf_keywords_values t inner join textrank_keywords_values r where t.keywords=r.keyword group by article_id2\"\"\"\n",
    "article_topics = ktt.spark.sql(topic_sql)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_profile = keywords_info.join(article_topics,keywords_info.article_id==article_topics.article_id2).select(['article_id','channel_id','keywords','topics'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleprofile.write.insertInto('article_profile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
