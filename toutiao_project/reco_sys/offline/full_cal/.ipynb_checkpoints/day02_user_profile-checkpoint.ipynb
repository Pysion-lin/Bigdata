{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# 如果当前代码文件运行测试需要加入修改路径，避免出现后导包问题\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sys.path.insert(0, os.path.join(BASE_DIR))\n",
    "\n",
    "PYSPARK_PYTHON = \"/miniconda2/envs/reco_sys/bin/python\"\n",
    "# 当存在多个版本时，不指定很可能会导致出错\n",
    "os.environ[\"PYSPARK_PYTHON\"] = PYSPARK_PYTHON\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = PYSPARK_PYTHON\n",
    "\n",
    "from offline import SparkSessionBase\n",
    "import pyhdfs\n",
    "import time\n",
    "\n",
    "\n",
    "class UpdateUserProfile(SparkSessionBase):\n",
    "    \"\"\"离线相关处理程序\n",
    "    \"\"\"\n",
    "    SPARK_APP_NAME = \"updateUser\"\n",
    "    ENABLE_HIVE_SUPPORT = True\n",
    "\n",
    "    SPARK_EXECUTOR_MEMORY = \"3g\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.spark = self._create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uup = UpdateUserProfile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uup.spark.sql('use profile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取日志数据，（关联历史日志数据和HIVE表分区）\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def datelist(startdate, enddate):\n",
    "    date = [datetime.strftime(d, '%Y-%m-%d') for d in list(pd.date_range(start=startdate, end=enddate))]\n",
    "    return date\n",
    "\n",
    "dl = datelist('2019-03-05', time.strftime('%Y-%m-%d', time.localtime()))\n",
    "\n",
    "\n",
    "pydfs = pyhdfs.HdfsClient(hosts=\"hadoop-master:50070\")\n",
    "# 循环每个日期进行关联\n",
    "for d in dl:\n",
    "    \n",
    "    # 构造hadoop地址\n",
    "    _location = '/user/hive/warehouse/profile.db/user_action/' + d\n",
    "    try:\n",
    "        if pydfs.exists(_location):\n",
    "            uup.spark.sql(\"alter table user_action add partition (dt='%s') location '%s'\" % (d, _location))\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果hadoop没有今天该日期文件，则没有日志数据，结束\n",
    "time_str = time.strftime(\"%Y-%m-%d\", time.localtime())\n",
    "_localions = '/user/hive/warehouse/profile.db/user_action/' + time_str\n",
    "if pydfs.exists(_localions):\n",
    "    # 如果有该文件直接关联，捕获关联重复异常\n",
    "    try:\n",
    "        uup.spark.sql(\"alter table user_action add partition (dt='%s') location '%s'\" % (time_str, _localions))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    partition|\n",
      "+-------------+\n",
      "|dt=2019-03-05|\n",
      "|dt=2019-03-06|\n",
      "|dt=2019-03-07|\n",
      "|dt=2019-03-08|\n",
      "|dt=2019-03-09|\n",
      "|dt=2019-03-10|\n",
      "|dt=2019-03-11|\n",
      "|dt=2019-03-12|\n",
      "|dt=2019-03-13|\n",
      "|dt=2019-03-14|\n",
      "|dt=2019-03-15|\n",
      "|dt=2019-03-16|\n",
      "|dt=2019-03-17|\n",
      "|dt=2019-03-18|\n",
      "|dt=2019-03-19|\n",
      "|dt=2019-03-20|\n",
      "|dt=2019-03-21|\n",
      "|dt=2019-03-22|\n",
      "|dt=2019-03-23|\n",
      "|dt=2019-03-24|\n",
      "|dt=2019-03-25|\n",
      "|dt=2019-03-26|\n",
      "|dt=2019-03-27|\n",
      "|dt=2019-03-28|\n",
      "|dt=2019-03-29|\n",
      "|dt=2019-03-30|\n",
      "|dt=2019-03-31|\n",
      "|dt=2019-04-01|\n",
      "|dt=2019-04-02|\n",
      "|dt=2019-04-03|\n",
      "|dt=2019-04-04|\n",
      "|dt=2019-04-05|\n",
      "|dt=2019-04-06|\n",
      "|dt=2019-04-07|\n",
      "|dt=2019-04-08|\n",
      "|dt=2019-04-09|\n",
      "|dt=2019-04-10|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 首先查看一下user_action中的分区\n",
    "uup.spark.sql(\"show partitions user_action\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先删除没有数据的分区，否则直接查询数据会报错\n",
    "# uup.spark.sql(\"alter table user_action drop partition(dt>'2019-04-10')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    partition|\n",
      "+-------------+\n",
      "|dt=2019-03-05|\n",
      "|dt=2019-03-06|\n",
      "|dt=2019-03-07|\n",
      "|dt=2019-03-08|\n",
      "|dt=2019-03-09|\n",
      "|dt=2019-03-10|\n",
      "|dt=2019-03-11|\n",
      "|dt=2019-03-12|\n",
      "|dt=2019-03-13|\n",
      "|dt=2019-03-14|\n",
      "|dt=2019-03-15|\n",
      "|dt=2019-03-16|\n",
      "|dt=2019-03-17|\n",
      "|dt=2019-03-18|\n",
      "|dt=2019-03-19|\n",
      "|dt=2019-03-20|\n",
      "|dt=2019-03-21|\n",
      "|dt=2019-03-22|\n",
      "|dt=2019-03-23|\n",
      "|dt=2019-03-24|\n",
      "|dt=2019-03-25|\n",
      "|dt=2019-03-26|\n",
      "|dt=2019-03-27|\n",
      "|dt=2019-03-28|\n",
      "|dt=2019-03-29|\n",
      "|dt=2019-03-30|\n",
      "|dt=2019-03-31|\n",
      "|dt=2019-04-01|\n",
      "|dt=2019-04-02|\n",
      "|dt=2019-04-03|\n",
      "|dt=2019-04-04|\n",
      "|dt=2019-04-05|\n",
      "|dt=2019-04-06|\n",
      "|dt=2019-04-07|\n",
      "|dt=2019-04-08|\n",
      "|dt=2019-04-09|\n",
      "|dt=2019-04-10|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 再查看删除分区之后的user_action中的分区\n",
    "uup.spark.sql(\"show partitions user_action\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = uup.spark.sql(\n",
    "\"select actionTime, readTime, channelId, param.articleId, param.algorithmCombine, param.action, param.userId from user_action where dt>='2019-04-01'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+---------+--------------------+----------------+--------+-------------------+\n",
      "|         actionTime|readTime|channelId|           articleId|algorithmCombine|  action|             userId|\n",
      "+-------------------+--------+---------+--------------------+----------------+--------+-------------------+\n",
      "|2019-04-02 12:21:55|        |        0|[44737, 44739, 14...|              C2|exposure|1112727762809913344|\n",
      "|2019-04-02 12:21:57|        |       18|              140357|              C2|   click|1112727762809913344|\n",
      "|2019-04-02 12:22:20|        |       18|              140357|              C2| collect|1112727762809913344|\n",
      "|2019-04-02 12:22:36|   38000|       18|              140357|              C2|    read|1112727762809913344|\n",
      "|2019-04-02 12:22:43|        |       18|               13476|              C2|   click|1112727762809913344|\n",
      "|2019-04-02 12:23:08|   23306|       18|               13476|              C2|    read|1112727762809913344|\n",
      "|2019-04-02 12:23:13|        |        0|[14805, 15196, 44...|              C2|exposure|1112727762809913344|\n",
      "|2019-04-02 12:23:16|        |        0|[18795, 18156, 43...|              C2|exposure|1112727762809913344|\n",
      "|2019-04-02 12:21:55|        |        0|[44737, 44739, 14...|              C2|exposure|1112727762809913344|\n",
      "|2019-04-02 12:21:57|        |       18|              140357|              C2|   click|1112727762809913344|\n",
      "|2019-04-02 12:22:20|        |       18|              140357|              C2| collect|1112727762809913344|\n",
      "|2019-04-02 12:22:36|   38000|       18|              140357|              C2|    read|1112727762809913344|\n",
      "|2019-04-02 12:22:43|        |       18|               13476|              C2|   click|1112727762809913344|\n",
      "|2019-04-02 12:23:08|   23306|       18|               13476|              C2|    read|1112727762809913344|\n",
      "|2019-04-02 12:23:13|        |        0|[14805, 15196, 44...|              C2|exposure|1112727762809913344|\n",
      "|2019-04-02 12:23:16|        |        0|[18795, 18156, 43...|              C2|exposure|1112727762809913344|\n",
      "|2019-04-02 12:21:55|        |        0|[44737, 44739, 14...|              C2|exposure|1112727762809913344|\n",
      "|2019-04-02 12:21:57|        |       18|              140357|              C2|   click|1112727762809913344|\n",
      "|2019-04-02 12:22:20|        |       18|              140357|              C2| collect|1112727762809913344|\n",
      "|2019-04-02 12:22:36|   38000|       18|              140357|              C2|    read|1112727762809913344|\n",
      "+-------------------+--------+---------+--------------------+----------------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转化格式：[\"user_id\", \"action_time\",\"article_id\", \"channel_id\", \"shared\", \"clicked\", \"collected\", \"exposure\", \"read_time\"]\n",
    "\n",
    "def _compute(row):\n",
    "    # 进行判断行为类型\n",
    "    _list = []\n",
    "    if row.action == \"exposure\":\n",
    "        for article_id in eval(row.articleId):\n",
    "            _list.append(\n",
    "                [row.userId, row.actionTime, article_id, row.channelId, False, False, False, True, row.readTime])\n",
    "        return _list\n",
    "    else:\n",
    "        class Temp(object):\n",
    "            shared = False\n",
    "            clicked = False\n",
    "            collected = False\n",
    "            read_time = \"\"\n",
    "\n",
    "        _tp = Temp()\n",
    "        if row.action == \"share\":\n",
    "            _tp.shared = True\n",
    "        elif row.action == \"click\":\n",
    "            _tp.clicked = True\n",
    "        elif row.action == \"collect\":\n",
    "            _tp.collected = True\n",
    "        elif row.action == \"read\":\n",
    "            _tp.clicked = True\n",
    "        else:\n",
    "            pass\n",
    "        _list.append(\n",
    "            [row.userId, row.actionTime, int(row.articleId), row.channelId, _tp.shared, _tp.clicked, _tp.collected,\n",
    "             True,\n",
    "             row.readTime])\n",
    "        return _list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对原user_action数据进行格式转换\n",
    "res = sqlDF.rdd.flatMap(_compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = res.toDF([\"user_id\", \"action_time\",\"article_id\", \"channel_id\", \"shared\", \"clicked\", \"collected\", \"exposure\", \"read_time\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+----------+------+-------+---------+--------+---------+\n",
      "|user_id|        action_time|         article_id|channel_id|shared|clicked|collected|exposure|read_time|\n",
      "+-------+-------------------+-------------------+----------+------+-------+---------+--------+---------+\n",
      "|      4|2019-04-01 09:27:10|1112525856586072064|         7| false|  false|    false|    true|         |\n",
      "|      4|2019-04-01 09:27:10|1112525856586072064|         7| false|  false|    false|    true|         |\n",
      "|      4|2019-04-01 09:27:15|1112525856586072064|         7| false|   true|    false|    true|         |\n",
      "|      4|2019-04-01 09:27:15|1112525856586072064|         7| false|   true|    false|    true|         |\n",
      "|      4|2019-04-01 09:27:38|1112525856586072064|         7| false|   true|    false|    true|    21828|\n",
      "|      4|2019-04-01 09:27:42|1112525856586072064|         7| false|   true|    false|    true|         |\n",
      "|      4|2019-04-01 09:27:38|1112525856586072064|         7| false|   true|    false|    true|    21828|\n",
      "|      4|2019-04-01 09:27:42|1112525856586072064|         7| false|   true|    false|    true|         |\n",
      "|      4|2019-04-01 09:27:47|1112525856586072064|         7| false|   true|    false|    true|     4331|\n",
      "|      4|2019-04-01 09:27:47|1112525856586072064|         7| false|   true|    false|    true|     4331|\n",
      "|      4|2019-04-01 09:27:48|1112525856586072064|         7| false|   true|    false|    true|         |\n",
      "|      4|2019-04-01 09:27:51|1112525856586072064|         7| false|   true|    false|    true|     2377|\n",
      "|      4|2019-04-01 09:27:48|1112525856586072064|         7| false|   true|    false|    true|         |\n",
      "|      4|2019-04-01 09:27:51|1112525856586072064|         7| false|   true|    false|    true|     2377|\n",
      "|      4|2019-04-01 09:27:10|1112525856586072064|         7| false|  false|    false|    true|         |\n",
      "|      4|2019-04-01 09:27:15|1112525856586072064|         7| false|   true|    false|    true|         |\n",
      "|      4|2019-04-01 09:27:10|1112525856586072064|         7| false|  false|    false|    true|         |\n",
      "|      4|2019-04-01 09:27:38|1112525856586072064|         7| false|   true|    false|    true|    21828|\n",
      "|      4|2019-04-01 09:27:42|1112525856586072064|         7| false|   true|    false|    true|         |\n",
      "|      4|2019-04-01 09:27:15|1112525856586072064|         7| false|   true|    false|    true|         |\n",
      "+-------+-------------------+-------------------+----------+------+-------+---------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并历史数据，插入表中\n",
    "old = uup.spark.sql(\"select * from user_article_basic\")\n",
    "# 由于合并的结果中不是对于user_id和article_id唯一的，一个用户会对文章有多种操作\n",
    "new_old = old.unionAll(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_old.registerTempTable(\"temptable\")\n",
    "# 将历史合并数据按照用户，文章分组后存放\n",
    "\n",
    "# uup.spark.sql(\n",
    "#         \"insert overwrite table user_article_basic select user_id, max(action_time) as action_time, \"\n",
    "#         \"article_id, max(channel_id) as channel_id, max(shared) as shared, max(clicked) as clicked, \"\n",
    "#         \"max(collected) as collected, max(exposure) as exposure, max(read_time) as read_time from temptable \"\n",
    "#         \"group by user_id, article_id\")\n",
    "\n",
    "# 通过这个数据表我们就能够知道用户对某篇文章的历史行为都有哪些\n",
    "# 注意：数据表user_article_basic中的数据我们已经有了，不需要再去写入存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
